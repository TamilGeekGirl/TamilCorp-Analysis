{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1327c0c6-dc57-4723-b7e9-d622072f566e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "# Pre-process abbreviations in Tamil \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d1be41f-2e56-4ae3-9f5a-7ddb300cb130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tamil_other_abbreviation_pattern = re.compile(r'(?:[\\u0B80-\\u0BFF]{1,3}\\.){2,}')\n",
    "tamil_month_abbreviations = [r'ஜன\\.', r'ஜனவரி\\.', r'பிப்\\.', r'பிப்ரவரி\\.', r'மார்ச்\\.', r'ஏப்\\.', r'ஏப்ரல்\\.', r'மே\\.', r'ஜூன்\\.', r'ஜூலை\\.', r'ஆக\\.', r'ஆகஸ்ட்\\.', r'ஆகஸ்டு\\.', r'ஆகத்து\\.', r'செப்\\.', r'செப்டம்பர்\\.' , r'அக்\\.', r'அக்டோபர்\\.', r'நவ\\.', r'நவம்பர்\\.', r'டிச\\.', r'டிசம்பர்\\.']\n",
    "sentence_endings = re.compile(r'(?<=[.!?])\\s')\n",
    "tamil_month_abbreviation_pattern = re.compile(r'|'.join(tamil_month_abbreviations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46478d29-78d2-4205-a4b2-aac0ae55c27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processAbbreviations(chunk):\n",
    "    \"\"\"\n",
    "    Function to process abbreviations within a chunk of text.\n",
    "    Ensures that abbreviations are not misinterpreted as sentence boundaries.\n",
    "    \"\"\"\n",
    "   \n",
    "    month_matches = list(tamil_month_abbreviation_pattern.finditer(chunk))\n",
    "    #print(month_matches)\n",
    "    other_matches = list(tamil_other_abbreviation_pattern.finditer(chunk))\n",
    "    matches = month_matches+other_matches\n",
    "    #print(matches)\n",
    "    for match in matches:\n",
    "        \n",
    "        abbr = match.group(0)\n",
    "        #print(abbr)\n",
    "        chunk = chunk.replace(abbr, abbr.replace('.', '<<PERIOD>>'))\n",
    "\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6681512-9a7a-4991-b088-3d936a2966ef",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "### Process abbreviations and write every sentence in a new line of a df.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6de9404b-afc1-4a1f-a727-77e434f0a020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NUM>ஆம் ஆண்டு வாழ. <NUM> பரிசுகள் கொண்டு வா.\n"
     ]
    }
   ],
   "source": [
    "s = \"1980ஆம் ஆண்டு வாழ. 80 பரிசுகள் கொண்டு வா.\"\n",
    "st = re.sub('\\d+', '<NUM>', s)\n",
    "print(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38968ec3-2341-4d4d-b211-c90785366d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted_chars_pattern = re.compile(r'[\\u200B-\\u200D\\uFEFF\\uFFFC]|[^\\w\\s.\\u0B80-\\u0BFF]')\n",
    "def read_tamil_sentences(file_path, output_file, chunk_size):  \n",
    "    with open(file_path, 'r', encoding='utf-8') as file, open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "        while True:\n",
    "            \n",
    "            chunk = file.read(chunk_size)\n",
    "\n",
    "            if not chunk:  \n",
    "                break\n",
    "\n",
    "            chunk = processAbbreviations(chunk)\n",
    "            sentences = sentence_endings.split(chunk)\n",
    "\n",
    "            \n",
    "            for sentence in sentences:\n",
    "                if sentence:  \n",
    "                    sent = re.sub('\\d+', '<NUM>', sentence)\n",
    "                    sent = sent.replace(\"�\",\"\")\n",
    "                    sent = re. sub(unwanted_chars_pattern,'',sent)\n",
    "                    out_file.write(sent + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "688580eb-14a2-43fd-8516-997250dd0f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences have been written to tamilSentences.csv\n",
      "The time of execution of above program is :  3160204.908847809 ms\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start = time.time()\n",
    "file_path = 'E:\\\\CORPUS WITH TXT AND TEI XML FILES\\\\TamilCorpus.txt'  \n",
    "output_file = 'tamilSentences.csv'  \n",
    "chunk_size = 1024*1024 # In 1024 KB or 1 MB chunks\n",
    "read_tamil_sentences(file_path, output_file, chunk_size)\n",
    "print(f\"Sentences have been written to {output_file}\")\n",
    "end = time.time()\n",
    "print(\"The time of execution of above program is : \",(end-start) * 10**3, \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d98772-c66f-451e-a30e-07fe76e3cc7a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "# Build a Tamil dictionary  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27c37a75-f201-466c-aa04-fb709c35f01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langid.langid import LanguageIdentifier, model\n",
    "identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4718ea88-6718-4ee9-abfe-68874bdb7a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Tamil words saved to E:\\tamilDictionary.json\n",
      "The time of execution of above program is :  841787.9774570465 ms\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "import time \n",
    "start = time.time()\n",
    "# Load the CSV file\n",
    "csv_file = \"tamilSentences.csv\"  \n",
    "df = pd.read_csv(csv_file)\n",
    "sentences = df.iloc[:, 0].dropna().tolist()\n",
    "uniqueTamilWords = set()\n",
    "for sentence in sentences:\n",
    "    words = sentence.split()\n",
    "    uniqueTamilWords.update(words)\n",
    "wordList = sorted(uniqueTamilWords)\n",
    "json_file = \"E:\\\\tamilDictionary.json\"\n",
    "with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(wordList, f, ensure_ascii=False, indent=4)\n",
    "print(f\"Unique Tamil words saved to {json_file}\")\n",
    "end = time.time()\n",
    "print(\"The time of execution of above program is : \",(end-start) * 10**3, \"ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f877d35d-7c01-4c93-8906-49e3fc8a4e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed lines saved to E:\\output.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def process_line(line):\n",
    "    # 1) Remove all dots (.)\n",
    "    line = line.replace(\".\", \"\")\n",
    "    \n",
    "    # 2) Remove all English characters except \"NUM\" and \"PERIOD\"\n",
    "    # Allow \"NUM\" and \"PERIOD\" (any number of occurrences) and remove everything else\n",
    "    line = re.sub(r\"(?!NUM|PERIOD)[A-Za-z]\", \"\", line)\n",
    "    \n",
    "    # Remove any extra spaces that might result\n",
    "    return line.strip()\n",
    "\n",
    "# File paths\n",
    "input_file = \"E:\\\\tamilDictionary.json\"  # Replace with your input JSON file\n",
    "output_file = \"E:\\\\output.json\"  # Replace with your desired output JSON file\n",
    "\n",
    "# Process the JSON file\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    output_lines = []\n",
    "    for line in infile:\n",
    "        line = line.strip()  # Remove leading/trailing whitespace\n",
    "        if line:  # Process non-empty lines\n",
    "            processed_line = process_line(line)\n",
    "            output_lines.append(processed_line)\n",
    "    \n",
    "    # Write all processed lines to the output file in JSON format\n",
    "    json.dump(output_lines, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Processed lines saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179cf184-794e-4222-9df3-3c3f3baef7ce",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "# Build a Word2Vec model \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6059be03-d14b-46f4-a36e-8b9b093a31f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6859326-1202-419c-9825-8c361d889c75",
   "metadata": {},
   "source": [
    "### Step 1: Load the Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3e5f4ff-18a0-42ef-bbfb-224038b5ce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'D:\\\\Preprocessed\\\\TamilCorpusSampled615.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text_data = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bafc6a-d09e-4bda-9787-f66a48b32bca",
   "metadata": {},
   "source": [
    "### Step 2: Tokenize Sentences with indic-nlp-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad69a7d5-1f8f-439f-8898-9191301b8d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.tokenize import indic_tokenize\n",
    "\n",
    "sentences = text_data.splitlines()  \n",
    "tokenized_sentences = [list(indic_tokenize.trivial_tokenize(sentence, lang='ta')) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f855587-8682-4d2c-b2a2-369991b74a35",
   "metadata": {},
   "source": [
    "### Step 3: Train the Word2Vec Model (skipgram model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3cf52a46-5e3d-497c-bd3a-69e8f2f55336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 4149013\n",
      "Top 50 words in the vocabulary: ['ஒரு', 'என்று', 'மற்றும்\\u200c', 'மொத்தம்\\u200c', 'இந்த', 'அல்லது', 'அரசு', 'வேண்டும்\\u200c', 'என்ற', 'அந்த', 'கொண்டு', 'மற்றும்', 'உள்ள', 'ரூ', 'இது', 'பழங்குடியினர்\\u200c', 'என்பது', 'பல', 'என', 'செய்து', 'சென்னை', 'வரவுகள்\\u200c', 'மாவட்ட', 'ஏனைய', 'தனது', 'அது', 'போன்ற', 'மதிப்பீடு', 'வேண்டும்', 'இருந்து', 'விடுதி', 'பெருந்தலைப்பு', 'ஆதிதிராவிடர்\\u200c', 'என்ன', 'திட்ட', 'அரியலூர்', 'பிரிவு', 'போது', 'நல', 'கல்வி', 'இல்லை', 'துறை', 'துணை', 'சில', 'கொண்ட', 'திரு', 'நான்\\u200c', 'மூலம்\\u200c', 'அவர்', 'தான்']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# vector_size: Each word will be represented as a dense vector with 200 components.\n",
    "# window: The model considers 5 words before and 5 words after a target word to predict word relationships.\n",
    "# min_count: Ignores words that appear fewer than 2 times in the corpus.\n",
    "# workers: Specifies the number of worker threads to use for parallel training.\n",
    "# sg=1: Skip-Gram model (focuses on predicting context words from a target word).\n",
    "sg_model = Word2Vec(sentences=tokenized_sentences, vector_size=300, window=5, min_count=2, workers=4, sg=1)\n",
    "sg_model.save(\"E:\\\\word2Vec model\\\\tamilWord2vec_SG.model\")\n",
    "print(f\"total words: {sg_model.corpus_total_words}\")\n",
    "# Display the top 50 words in the vocabulary\n",
    "top_50_words = list(sg_model.wv.index_to_key[1:51])\n",
    "print(\"Top 50 words in the vocabulary:\", top_50_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8a5946-bb16-4504-a32a-92885f345582",
   "metadata": {},
   "source": [
    "### Step 3.1: Train the Word2Vec Model (CBOW model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "befe28fa-2e5f-4a02-b859-29ff1eb41518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words: 4149013\n",
      "Top 50 words in the vocabulary: ['ஒரு', 'என்று', 'மற்றும்\\u200c', 'மொத்தம்\\u200c', 'இந்த', 'அல்லது', 'அரசு', 'வேண்டும்\\u200c', 'என்ற', 'அந்த', 'கொண்டு', 'மற்றும்', 'உள்ள', 'ரூ', 'இது', 'பழங்குடியினர்\\u200c', 'என்பது', 'பல', 'என', 'செய்து', 'சென்னை', 'வரவுகள்\\u200c', 'மாவட்ட', 'ஏனைய', 'தனது', 'அது', 'போன்ற', 'மதிப்பீடு', 'வேண்டும்', 'இருந்து', 'விடுதி', 'பெருந்தலைப்பு', 'ஆதிதிராவிடர்\\u200c', 'என்ன', 'திட்ட', 'அரியலூர்', 'பிரிவு', 'போது', 'நல', 'கல்வி', 'இல்லை', 'துறை', 'துணை', 'சில', 'கொண்ட', 'திரு', 'நான்\\u200c', 'மூலம்\\u200c', 'அவர்', 'தான்']\n"
     ]
    }
   ],
   "source": [
    "# sg=0: CBOW model - Continuous Bag of Words, predicts a target word from surrounding context words\n",
    "cbow_model = Word2Vec(sentences=tokenized_sentences, vector_size=200, window=5, min_count=2, workers=4, sg=0)\n",
    "cbow_model.save(\"E:\\\\word2Vec model\\\\tamilWord2vec_CBOW.model\")\n",
    "print(f\"total words: {cbow_model.corpus_total_words}\")\n",
    "# Display the top 50 words in the vocabulary\n",
    "top_50_words = list(cbow_model.wv.index_to_key[1:51])\n",
    "print(\"Top 50 words in the vocabulary:\", top_50_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76beb22-d791-4029-8e26-aff696d8cbe7",
   "metadata": {},
   "source": [
    "### Step 4: Analyze word similarities (An example using skipgram model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2fd11b0e-26fb-416a-bd25-9b13cace7278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to 'சட்ட':\n",
      "இந்திய: 0.9664392471313477\n",
      "குற்றவியல்‌: 0.9620451927185059\n",
      "செலவினமாகும்‌: 0.9597854018211365\n",
      "விளக்கம்‌: 0.9593039751052856\n",
      "பாதிக்கப்பட்டவருக்கு: 0.9582811594009399\n",
      "சட்டப்‌: 0.9575212597846985\n",
      "தண்டனைச்‌: 0.956930935382843\n",
      "நுணுக்க: 0.9562482237815857\n",
      "சட்டநடைமுறைகளில்‌: 0.9562100768089294\n",
      "சாசன: 0.952883243560791\n",
      "குற்றத்தினால்‌: 0.9522219300270081\n",
      "நாணயம்‌: 0.9518848061561584\n",
      "விரோதமான: 0.9510866403579712\n",
      "தேவைப்‌: 0.9506672024726868\n",
      "சம்பந்தப்பட்டவர்கள்‌: 0.9494932889938354\n"
     ]
    }
   ],
   "source": [
    "word = 'சட்ட'  \n",
    "if word in sg_model.wv.key_to_index:\n",
    "    similar_words = sg_model.wv.most_similar(word, topn=15)\n",
    "    print(f\"Words most similar to '{word}':\")\n",
    "    for sim_word, score in similar_words:\n",
    "        print(f\"{sim_word}: {score}\")\n",
    "else:\n",
    "    print(f\"Word '{word}' not found in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd98cf7-8df5-46e6-8794-5e1107899aa7",
   "metadata": {},
   "source": [
    "### Step 4.1: Analyze word similarities (An example using CBOW model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b49bfe99-ee3c-4cb3-909b-338ace3f78d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to 'துறை':\n",
      "கணக்கு: 0.9991927146911621\n",
      "தலைவர்‌: 0.9989467859268188\n",
      "முதலமைச்சர்‌: 0.9986269474029541\n",
      "மாண்புமிகு: 0.998553991317749\n",
      "செயலாளரின்‌: 0.9984923601150513\n",
      "அமைச்சர்‌: 0.9979434609413147\n",
      "செயலகம்‌: 0.9976032376289368\n",
      "திருவள்ளூர்‌: 0.9974076151847839\n",
      "கழகம்‌: 0.9972704648971558\n",
      "தாட்கோ: 0.9969611763954163\n",
      "இயக்குநருக்கு: 0.9969244599342346\n",
      "கட்டடங்கள்‌: 0.9967366456985474\n",
      "அமைச்சரின்‌: 0.9967193603515625\n",
      "இருப்புக்‌: 0.9967154860496521\n",
      "அலுவலகம்‌: 0.9966514706611633\n"
     ]
    }
   ],
   "source": [
    "word = 'துறை'  \n",
    "if word in cbow_model.wv.key_to_index:\n",
    "    similar_words = cbow_model.wv.most_similar(word, topn=15)\n",
    "    print(f\"Words most similar to '{word}':\")\n",
    "    for sim_word, score in similar_words:\n",
    "        print(f\"{sim_word}: {score}\")\n",
    "else:\n",
    "    print(f\"Word '{word}' not found in the vocabulary.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YazhRecentGPU",
   "language": "python",
   "name": "yazhgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
